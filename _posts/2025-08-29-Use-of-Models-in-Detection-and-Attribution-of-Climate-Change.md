---
layout: post
title: "Use of_Models in Detection and Attribution of Climate Change"
date: 2025-08-29 21:00:00 +0900
tags: [Article, D&A]
---

## 1. INTRODUCTION

The primary objective of Detection and Attribution (D&A) science is to differentiate between changes originating from external influences and those that are part of the climate's natural, internal behavior. The majority of D&A studies employ climate models to project the anticipated "fingerprint" of climate change and to gauge the level of uncertainty in that projection when compared to real-world observations. In essence, these studies assess how well the response patterns simulated by models can account for the climate changes that have been observed.

These methods work by breaking down observed climate change into two distinct categories:
* **Internal variability**: Fluctuations that are naturally generated within the climate system, such as the El Niño-Southern Oscillation.
* **External forcings**: Changes driven by factors outside of the climate system. Common examples are shifts in the Earth's energy balance caused by:
    * Increasing greenhouse gas concentrations, which alter how infrared radiation escapes to space.
    * Fluctuations in the amount of energy received from the sun.
    * Sporadic, large-scale volcanic eruptions.

Any such change to the planet’s energy balance is referred to as 'radiative forcing'. A major purpose of D&A is to provide an objective evaluation of how well climate models can replicate observed changes, to determine the relative importance of external drivers versus natural variability, and to help make future climate predictions that are anchored in the changes already recorded.

It's important to note, however, that D&A research is subject to uncertainties that go beyond the models themselves. The observational data has its own uncertainties, as does our knowledge of radiative forcing. While the forcing from well-mixed greenhouse gases is understood with high confidence, other human-caused forcings, like those from aerosols or changes in land use, carry greater uncertainty. This picture is made more complex by feedback loops within the climate system, which can either magnify or reduce the initial impact of a given forcing.

> **Section Summary:** Detection and Attribution (D&A) is the science of separating naturally occurring climate variability from changes caused by external factors like greenhouse gases. Climate models are essential tools in this process, as they provide the predicted "fingerprints" of these external forcings. However, the accuracy of D&A studies is affected by uncertainties in both the forcings themselves and the models used to simulate their effects.

---

## 2. OBSERVATION-ONLY METHODS FOR IDENTIFYING THE ANTHROPOGENIC SIGNAL

Some scientific efforts have aimed to isolate the signals of human-caused and other external forcings using only observational records. Although this is an appealing approach, it demands significant assumptions to separate an externally driven trend from the climate's natural fluctuations.

### 2.1. Methods Using Spatial Patterns
One study by Wallace et al. noted that a significant portion of winter climate behavior in the Northern Hemisphere is linked to a spatial pattern known as "Cold Ocean Warm Land" (COWL). By statistically removing the variability tied to this pattern, a residual large-scale temperature trend emerges, which is thought to be a clearer representation of the response to an external forcing. The main drawback, however, is that the COWL pattern itself shares key characteristics with the expected fingerprint of greenhouse warming—specifically, the contrast between faster warming over land than oceans.

{% raw %}
<img src="/images/Use_of_Models_in_Detection_and_Attribution_of_Climate_Change/wires_2011_1_top.png" alt="Figure 1 top">
{% endraw %}
**Figure 1 top**: Results of studies filtering externally driven signals based on observational data only. A time series of global mean monthly surface temperature (topmost) is shown compared to contributions by variability from El Nino, short-term dynamical variations in extratropics (Tdyn), and the residual that remains after removing both (each time series is offset for presentation purposes; Reprinted with permission from Ref 18.
Copyright 2009 American Meteorological Society). The vertical lines indicate August 1945 and the timing of volcanic eruptions. 

### 2.2. Methods Separating Signal and Noise Based on Timescales
Another set of studies has tried to untangle the influence of long-term external drivers from short-term climate dynamics by concentrating on differences in their timescales. For instance, this can involve looking for a persistent trend in a climate index that is typically dominated by short-term fluctuations, like the Arctic Oscillation. The core assumption here is that external drivers produce patterns of change that are statistically unique from the patterns of natural variability. The problem is that natural climate variability occurs across all timescales, from years to centuries, which means any such separation will be imperfect. Furthermore, this method can't easily distinguish between different long-term drivers, like human GHG emissions and slow changes in solar output.

{% raw %}
<img src="/images/Use_of_Models_in_Detection_and_Attribution_of_Climate_Change/wires_2011_1_bot.png" alt="Figure 1 bottom">
{% endraw %}
**Figure 1 bottom**: First discriminants of interdecadal variations in (a) January and (c) July, based on separating climate variability between long and short timescales. Changes are expressed relative to the 1916–1998 mean. Upper panels (a) and (c): discriminant pattern. Lower panels (b) and (d): canonical variates, which give the time evolution. (Reprinted with permission from Ref 20. Copyright 2001 American Meteorological Society)

### 2.3. Methods Using Forcing History
This technique is built on the fact that current atmospheric levels of greenhouse gases are far higher than at any point in the recent geological past. This inspires a comparison of modern temperatures with paleo-climate reconstructions spanning hundreds or thousands of years to see if the current era is anomalous. Yet, a simple comparison of different climate states has its limits, as natural variability and other natural forcings can also cause significant changes over long periods. Moreover, this approach is less effective when a forcing, like GHG emissions, is increasing very quickly. Due to the thermal inertia of the oceans, there is a significant lag before the full warming effect is felt.

In conclusion, while these observation-only techniques avoid making explicit assumptions about the climate's response, they rely on other strong, implicit assumptions—for example, that the climate reacts instantly to a forcing or that forced changes and natural variability can be neatly divided by timescale. In effect, they use their own simplified, implicit 'models' of how the climate system functions.

> **Section Summary:** Methods that use only observations to find a human-caused climate signal try to filter out natural "noise" by analyzing spatial patterns, separating long-term from short-term changes, or comparing today's climate to the distant past. While insightful, these approaches are limited because they rely on strong assumptions and cannot fully account for the complex physics of the climate system, such as natural variability that occurs over long timescales and the delayed warming effect caused by the oceans.

---

## 3. DETECTION AND ATTRIBUTION METHODS USING CLIMATE MODELS

Given the constraints of observation-only approaches, most research utilizes models grounded in physics to estimate the expected climate response to external drivers. These studies use "fingerprint methods," which employ model-simulated response patterns to measure the role of various forcings in the climate changes we have observed.

### 3.1. Methods Comparing Climate Model Data with Observations
The most straightforward application of climate models is to place the recent history of temperature changes side-by-side with simulations. Such direct comparisons have been used to provisionally attribute the warming of the last century to a mix of human and natural causes. As Figure 2 illustrates, complex climate models can successfully replicate the observed history of global temperature changes only when they incorporate both anthropogenic and natural forcings. When the models are run with only natural forcings, they cannot account for the pronounced warming of recent decades.

{% raw %}
<img src="/images/Use_of_Models_in_Detection_and_Attribution_of_Climate_Change/wires_2011_2.png" alt="Figure 2">
{% endraw %}
**Figure 2**:Comparison between global mean temperature changes relative to the 1901–1950 average (◦C) from observations (black) and simulated by climate model simulations that include (a) both human and natural influences on climate (for example, the effect of strong volcanic eruptions, marked by vertical gray bars) and (b) natural influences only. Individual model simulations are shown by thin lines, their average by a fat line (red in panel (a), blue in panel (b)). (Reprinted with permission from Ref 2. Copyright 2007 Cambridge University Press)


### 3.2. Fingerprint Methods
Fingerprint techniques are a more advanced approach that leverages both spatial and temporal data to better distinguish the effects of different forcings. This allows for the contribution of each individual signal to be estimated independently. Two primary statistical frameworks are used:

**Frequentist Approach.** Initial studies used a technique called **pattern correlation statistics**, which measures the spatial resemblance between a predicted pattern of climate change and the observed pattern. The limitation of this method is that it cannot be fine-tuned to better detect a weak signal, nor can it handle the simultaneous identification of responses to multiple forcings. Modern **optimal detection methods** treat the observed climate change ($y$) as a composite of several model-derived signals ($X$) plus some leftover natural climate noise ($u$):

$$ y = Xa + u $$

* The vector $y$ represents the observed climate data, which has been spatially and/or temporally filtered.
* The matrix $X$ holds the "fingerprints," with each column being the simulated response pattern for a particular external forcing.
* The vector $a$ is a set of scaling factors, which are calculated to determine how much the amplitude of each model fingerprint needs to be adjusted to best fit the observations.
* The vector $u$ is the residual, representing the portion of the observations not explained by the scaled fingerprints, and is treated as internal climate variability with a covariance matrix $C$.

A statistical technique provides a best linear unbiased estimator (BLUE) for these scaling factors. The scaling factors effectively correct for potential errors in either the magnitude of the external forcing or the model's sensitivity. If the calculated uncertainty for a scaling factor is positive and does not overlap with zero, the fingerprint is considered "detected."

{% raw %}
<img src="/images/Use_of_Models_in_Detection_and_Attribution_of_Climate_Change/wires_2011_3.png" alt="Figure 3">
{% endraw %}
**Figure 3**:Schematic for detection and attribution. The observed change (shown here: pattern of temperature change over the 20th century, left) is composed of a linear combination of fingerprints for all forcings combined (top, right) and for natural forcings only (center right, this combination allows rescaling of natural vs anthropogenic fingerprints in simulations of the 20th century) plus residual, unexplained variability. The resulting scaling factors and warming per fingerprint can be used to derive contributions to warming such as shown in the bottom panel, labeled
panel (c), although in this instance the latter is derived from three fingerprints. It shows attributable warming estimated from a detection and attribution analysis for the 20th century, using a fingerprint of the spatial pattern and time evolution of climate change forced by greenhouse gases
(red), other anthropogenic forcing (green), and solar and volcanic forcings combined (blue). The best estimate contribution of each forcing to warming in the 50-year period 1950–1999 is given by the vertical bar and the 5–95% uncertainty in that estimate is given by the black whiskers. The observed trend over that period is shown by a black horizontal line. The different estimates are derived using fingerprints from different models. (Reprinted with permission from Ref 2. Copyright 2007 Cambridge University Press)


**Bayesian Approaches.** Bayesian statistics are becoming more common in climate research as they offer a structured way to incorporate various sources of uncertainty into a single analysis. They also allow for the integration of information from different lines of evidence. Inferences in this framework are drawn from a posterior distribution, which is a combination of information from observations and prior knowledge.

### 3.3. Results of Large-Scale D&A Studies
D&A studies have been performed using a range of models, from simpler energy balance models (EBMs) to highly complex ones. Studies focusing on palaeoclimatic reconstructions of Northern Hemisphere temperature have shown that while a combination of solar, volcanic, and anthropogenic forcings shaped the past climate, greenhouse gas increases are essential to account for the warming in nearly all records of the last millennium. While volcanic effects were also detectable, a robust signal from solar forcing alone could not be identified. The ability to detect anthropogenic influence on variables other than temperature, such as precipitation and sea level pressure, has only been possible with the use of full-scale climate models.

{% raw %}
<img src="/images/Use_of_Models_in_Detection_and_Attribution_of_Climate_Change/wires_2011_4.png" alt="Figure 4">
{% endraw %}
**Figure 4**:(a) Energy balance model simulations of the response to greenhouse gas increases, moderated by aerosols in the 20th century (red),
solar forcing (green), and volcanic forcing (blue) (Reprinted with permission from Ref 58. Copyright 2003). (b) Results obtained using these simulations as fingerprints for the effect of all forcings combined in a palaeoclimatic reconstruction (black, fitted to best match the reconstruction with gray shading indicating the uncertainty in the scaling factor) compared to a fit of a coupled model (grey). The lower half of the panel shows the estimated contribution by each individual forcing time series scaled to match the reconstruction in a multiple regression, with shading again indicating the uncertainty in the scaling factor and hence in the estimated contribution by individual forcings. The uncertainty in the solar signal (green) is not shown as the effect of that forcing could not be distinguished from noise ; note that similar results are obtained using a number of other reconstructions, see paper). (Reprinted with permission from Ref 12. Copyright 2007 American Meteorological Society)


> **Section Summary:** Model-based D&A uses "fingerprint" methods to statistically compare observed climate patterns to model-simulated responses from various forcings. The primary technique, optimal detection, uses regression to calculate scaling factors that quantify how much each forcing has contributed to the observed changes. A signal is "detected" if its scaling factor is statistically different from zero. This powerful approach allows scientists to separate multiple climate change signals from the background noise of natural variability.

---

## 4. ROBUSTNESS OF RESULTS TO MODEL ERROR

Given that D&A techniques are highly reliant on model output, it is essential to evaluate whether their conclusions hold up against potential model deficiencies.

### 4.1. Robustness to Uncertainty in Fingerprints
The accuracy of a model's fingerprint can be affected by several factors:
* **Forcing uncertainties**, which are most significant for aerosols and solar activity.
* **Internal variability**, which can obscure the true signal and is commonly addressed by averaging over an ensemble of model runs.
* **Structural error**, which arises from the different ways models are built and parameterized. To minimize this, fingerprints are often constructed from the average of many different models (a multi-model mean).

Confidence in the results is bolstered when there are clear reasons why the signals from different forcings are separable. This is possible because they have very different signatures in both time and space:
* **Greenhouse Gas Forcing** has risen consistently, leading to a distinct pattern of greater warming over continents and in the Arctic.
* **Aerosol Forcing** peaked in the mid-20th century and has since leveled off or declined in many regions. Its effects are stronger in the Northern Hemisphere, which has slowed warming there more than in the south.
* **Solar Forcing** follows natural cycles of varying lengths.
* **Volcanic Forcing** is intermittent, causing sudden, distinct cooling events.

These unique characteristics help ensure the fingerprints are not "multi-collinear," which allows statistical methods to tell them apart. This separation process is more challenging for variables like precipitation, where model-simulated changes are often smaller than observed, and the impact of aerosols is highly uncertain.

### 4.2. Robustness to Uncertainty in Estimates of Climate Variability
Climate models also serve a vital function by providing estimates of the climate's natural internal variability (the "noise"). A flawed estimate of variability is a significant problem, as it directly impacts the calculated uncertainty of the scaling factors. The credibility of a model's simulated variability is cross-checked in several ways:
* First, the variability from model control runs is compared with the variability found in historical observations.
* Second, a "residual consistency test" is performed, which checks whether the variability left over after the regression analysis is compatible with the model's own estimate of its internal variability.
* Finally, the variability in paleo-climate reconstructions that is not explained by past external forcings can be used as another benchmark for comparison with model simulations.

> **Section Summary:** The conclusions of D&A studies are tested for robustness. Uncertainties in the model-generated "fingerprints" are managed by using multi-model ensembles and by exploiting the unique signatures of different forcings. Uncertainties in the models' simulation of natural "noise" are checked by rigorously comparing the model-simulated variability against observational records and statistical tests. For large-scale temperature, these checks confirm that the findings are robust.

---

## 5. TOWARD DETECTION AND ATTRIBUTION OF REGIONAL CHANGES AND IMPACT-RELEVANT CLIMATE VARIABLES

Research is increasingly shifting toward D&A at regional scales and for variables that are directly relevant to societal and ecological impacts, though this comes with considerable difficulties. The main problem is a lower signal-to-noise ratio; natural variability, which is large at local scales, makes it much harder to pick out the smaller signal of external forcing. This might be addressed with more advanced filtering techniques or by using information from neighboring regions.

For impact-related variables, progress will be aided by the growing resolution of climate models and a more detailed accounting of regional forcings like land-use change. However, challenges remain:
* **Precipitation:** Rain gauges measure rainfall at a specific point, but climate models simulate an average over a vast grid cell, creating a "scale mismatch."
* **Impacts:** Information on how species distributions or human health have changed over time is often sparse and inconsistent. Furthermore, these impacts are affected by many non-climatic "confounding factors" that are unrelated to climate change. It is critical to distinguish the effects of ongoing greenhouse gas-driven changes from these other factors.

{% raw %}
<img src="/images/Use_of_Models_in_Detection_and_Attribution_of_Climate_Change/wires_2011_6.png" alt="Figure 5">
{% endraw %}
**Figure 5**:(a) Comparison of precipitation in zonal latitude bands between a multi-model mean (blue time series, trend shown by red dashes) and observations (black, trend shown by black dashes), overlay colors indicate bands where both model and observations show increases (green), decreases (yellow), or neutral/disagreeing sign of change (gray), and (b) zonal pattern of variance ratio between climate models and observations. The figure shows box and whisker plots of the ratio of 5-year 10◦ zonal mean precipitation variances between all-forcing simulations and that estimated from station observations. The upper and lower ends of each box are drawn at the 75th and 25th quartiles, and the bar through each box is drawn at the median. The two bars indicate the range that would cover approximately 90% of variance ratios if the upper or lower halves of the variance ratio distribution were roughly Gaussian in shape. Individual points beyond the horizontal bars indicate outliers. (Copyright 2007 Nature Publishing Group)




The typical method involves using climate model output to understand changes in the physical climate system first, and then using a second model or statistical relationship to connect those physical changes to an impact variable.

> **Section Summary:** Applying D&A to regional scales and specific climate impacts is challenging due to higher natural variability (lower signal-to-noise) and the influence of non-climatic factors. Progress requires higher-resolution models and more sophisticated methods to isolate the climate change signal.

---

## CONCLUSION

The various techniques used in D&A research possess different levels of resilience to assumptions and model inaccuracies.
* **Simple methods** that compare changes are susceptible to errors in the model's climate sensitivity and the magnitude of the forcings.
* **Pattern-based methods** are not sensitive to the magnitude of the simulated response but can be affected by errors in its spatial or temporal pattern.
* **Regression-based techniques** offer key advantages: they can be optimized to improve the signal-to-noise ratio, they can analyze multiple signals at once, and they can formally account for uncertainty arising from both internal variability and model structure.

To better predict future climate and connect climate impacts to their ultimate drivers, we will need reliable D&A for smaller-scale changes in variables beyond temperature, which remains a difficult task. D&A studies do not just use climate models; they serve as a powerful tool for evaluating them. If the leftover residual from a regression is larger than expected, it calls into question the statistical assumptions or the model's simulation of fingerprints and variability. Similarly, if a signal is detected but its amplitude in observations is significantly different from what models simulate, it points to flaws in the model or the forcings used to drive it.

---

## Overview

This document provides a comprehensive overview of how scientists use climate models to detect and attribute observed climate change to specific causes, like human greenhouse gas emissions. It contrasts the limitations of using observational data alone with the more powerful "fingerprint" methods that rely on model simulations to separate signals of change from the noise of natural climate variability. The text delves into the statistical techniques used, such as optimal detection , and emphasizes the crucial process of testing these findings for robustness against uncertainties in both the models and the external forcings they represent. After reviewing the detailed methodologies and lines of evidence in Hegerl et al., I was impressed by the scientific rigor involved in climate attribution. The paper effectively demonstrates how confidence in the conclusion that humans are changing the climate is not based on a single piece of evidence, but is built through a careful, multi-faceted process of modeling, statistical analysis, and continuous validation against observations.

## Reference
- Hegerl, G., & Zwiers, F. (2011). Use of models in detection and attribution of climate change. Wiley Interdisciplinary Reviews: Climate Change, 2(4), 570-591. https://doi.org/10.1002/wcc.121
